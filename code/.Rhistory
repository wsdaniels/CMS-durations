new.leak.data$tc_ExpDurationHrs <- as.numeric(difftime(new.leak.data$tc_ExpEndDatetime, new.leak.data$tc_ExpStartDatetime, units = "hours"))
singlesource.leaks <- new.leak.data
n.leaks <- nrow(singlesource.leaks)
} # end if tree over run type
range(singlesource.leaks$tc_ExpDurationHrs)
range(singlesource.leaks$tc_C1MassFlow)
range(singlesource.leaks$tc_C1MassFlow)/1000
# Description: Makes plots for CMS durations manuscript
# Author: William Daniels (wdaniels@mines.edu)
# Last Updated: May 1, 2024
# Clear environment
if(!is.null(dev.list())){dev.off()}
rm(list = ls())
# Import necessary libraries
library(lubridate)
library(zoo)
library(scales)
if (commandArgs()[1] == "RStudio"){
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
}
pd.to.fd <- function(pd){
pd <- as.matrix(pd)
tmp <- pd + 1
tmp <- ifelse(tmp < 1, -1/tmp, tmp)
return(as.data.frame(tmp))
}
# START USER INPUT
#---------------------------------------------------------------------------
# Select experiment to run:
#   1) ADED 2022
#   2) ADED 2023
#   3) Stanford
run.type <- 1
# END OF USER INPUT - NO MODIFICATION NECESSARY BELOW THIS POINT
#---------------------------------------------------------------------------
if (run.type == 1){ n.r <- 8
} else if (run.type == 2){ n.r <- 10
} else if (run.type == 3){ n.r <- 6 }
slopes <- as.data.frame(matrix(NA, nrow = n.r, ncol = 6))
names(slopes) <- c("naive.lower", "naive", "naive.upper",
"proposed.lower", "proposed", "proposed.upper")
r2 <- as.data.frame(matrix(NA, nrow = n.r, ncol = 2))
names(r2) <- c("naive", "proposed")
percent.within.2x <- vector(length = n.r)
number.of.sensors <- 8
# Description: Makes plots for CMS durations manuscript
# Author: William Daniels (wdaniels@mines.edu)
# Last Updated: May 1, 2024
# Clear environment
if(!is.null(dev.list())){dev.off()}
rm(list = ls())
# Import necessary libraries
library(lubridate)
library(zoo)
library(scales)
if (commandArgs()[1] == "RStudio"){
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
}
pd.to.fd <- function(pd){
pd <- as.matrix(pd)
tmp <- pd + 1
tmp <- ifelse(tmp < 1, -1/tmp, tmp)
return(as.data.frame(tmp))
}
# START USER INPUT
#---------------------------------------------------------------------------
# Select experiment to run:
#   1) ADED 2022
#   2) ADED 2023
#   3) Stanford
run.type <- 2
# END OF USER INPUT - NO MODIFICATION NECESSARY BELOW THIS POINT
#---------------------------------------------------------------------------
if (run.type == 1){ n.r <- 8
} else if (run.type == 2){ n.r <- 10
} else if (run.type == 3){ n.r <- 6 }
slopes <- as.data.frame(matrix(NA, nrow = n.r, ncol = 6))
names(slopes) <- c("naive.lower", "naive", "naive.upper",
"proposed.lower", "proposed", "proposed.upper")
r2 <- as.data.frame(matrix(NA, nrow = n.r, ncol = 2))
names(r2) <- c("naive", "proposed")
percent.within.2x <- vector(length = n.r)
number.of.sensors <- 10
# Read in duration estimates
if (run.type == 1){
save.dir <- "../figures/ADED2022/"
data <- readRDS(paste0('../output_data/ADED2022_duration_estimates_best_arrangement_', number.of.sensors, '_sensors.RData'))
} else if (run.type == 2){
save.dir <- "../figures/ADED2023/"
data <- readRDS(paste0('../output_data/ADED2023_duration_estimates_best_arrangement_', number.of.sensors, '_sensors.RData'))
} else if (run.type == 3){
save.dir <- "../figures/Stanford/"
data <- readRDS(paste0('../output_data/Stanford_duration_estimates_best_arrangement_', number.of.sensors, '_sensors.RData'))
}
# Parse out results
all.durations          <- data$all.durations
info.list              <- data$info.list
spikes                 <- data$spikes
rate.est.all.events    <- data$rate.est.all.events
loc.est.all.events     <- data$loc.est.all.events
source.names           <- data$source.names
start.bounds           <- data$start.bounds
end.bounds             <- data$end.bounds
original.durations     <- data$original.durations
max.obs                <- data$max.obs
error.lower.all.events <- data$error.lower.all.events
error.upper.all.events <- data$error.upper.all.events
# Pull event "event numbers" that uniquely identify each naive event
event.nums <- na.omit(unique(spikes$events))
# Number of naive events
n.ints <- length(event.nums)
# Pull out times
times <- spikes$time
# Grab either ADED 2022 or 2023 leak data
if (run.type == 1){
leak.data <- suppressWarnings(read.csv('../input_data/ADED2022_leak_data.csv'))
} else if (run.type == 2){
leak.data <- suppressWarnings(read.csv('../input_data/ADED2023_leak_data.csv'))
} else if (run.type == 3){
leak.data <- readRDS('../input_data/Stanford_leak_data.RData')
}
if (run.type %in% c(1,2)){
# Rename equipment groups for clarity
leak.data$tc_EquipmentGroupID[leak.data$tc_EquipmentGroupID == "4T"] <- "Tank"
leak.data$tc_EquipmentGroupID[leak.data$tc_EquipmentGroupID == "4W"] <- "Wellhead.West"
leak.data$tc_EquipmentGroupID[leak.data$tc_EquipmentGroupID == "4S"] <- "Separator.West"
leak.data$tc_EquipmentGroupID[leak.data$tc_EquipmentGroupID == "5W"] <- "Wellhead.East"
leak.data$tc_EquipmentGroupID[leak.data$tc_EquipmentGroupID == "5S"] <- "Separator.East"
# Convert time strings to datetime objects
leak.data$tc_ExpStartDatetime <- as_datetime(as_datetime(leak.data$tc_ExpStartDatetime), tz = "America/Denver")
leak.data$tc_ExpEndDatetime   <- as_datetime(as_datetime(leak.data$tc_ExpEndDatetime),   tz = "America/Denver")
# Mask in leak data during simulation time frame
to.keep <- leak.data$tc_ExpStartDatetime >= times[1] & leak.data$tc_ExpStartDatetime <= times[length(times)]
leak.data <- leak.data[to.keep,]
# Sort by leak start time
leak.data <- leak.data[order(leak.data$tc_ExpStartDatetime), ]
# Mask for multisource leaks
multisource <- rep(F, nrow(leak.data))
multisource[leak.data$tc_ExpEPCount > 1] <- T
# Separate out the single and multisource leaks
singlesource.leaks <- leak.data[!multisource, ]
multisource.leaks <- leak.data[multisource, ]
# Number of METEC emission events
n.leaks <- nrow(singlesource.leaks)
# Initialize vector to hold estimated events that should be removed because
# they overlap with a multisource leak
to.remove <- vector(length = length(event.nums))
# Remove events that overlap with a multisource leak
for (i in 1:length(event.nums)){
# Mask in this event
this.spike <- which(spikes$events == event.nums[i])
these.times <- spikes$time[this.spike]
# Loop through multisource leaks
for (j in 1:nrow(multisource.leaks)){
leak.start <- multisource.leaks$tc_ExpStartDatetime[j]
leak.stop  <- multisource.leaks$tc_ExpEndDatetime[j]
# If there is any overlap between estimated event and leak, remove
# the event
if (any(these.times >= leak.start & these.times <= leak.stop)){
spikes$events[this.spike] <- NA
to.remove[i] <- T
break
}
}
}
# Get event numbers after filtering out events that overlap with multisource leaks
event.nums <- na.omit(unique(spikes$events))
# Number of events after filtering multisource leaks
n.ints <- length(event.nums)
# Filter out all naive events that occurred during a multi-source release
rate.est.all.events    <- rate.est.all.events[!to.remove]
error.lower.all.events <- error.lower.all.events[!to.remove]
error.upper.all.events <- error.upper.all.events[!to.remove]
loc.est.all.events     <- loc.est.all.events[!to.remove]
all.durations          <- all.durations[!to.remove]
start.bounds           <- start.bounds[!to.remove]
end.bounds             <- end.bounds[!to.remove]
original.durations     <- original.durations[!to.remove]
# Initialize vector to hold estimated events that should be removed because
# they overlap with a source not included in the study
to.remove <- vector(length = length(event.nums))
# Remove events that overlap with a source not included in the study
for (i in 1:length(event.nums)){
# Mask in this event
this.spike <- which(spikes$events == event.nums[i])
these.times <- spikes$time[this.spike]
# Loop through leaks from source not included in this study
for (j in which(singlesource.leaks$tc_EquipmentGroupID == "Underground")){
leak.start <- singlesource.leaks$tc_ExpStartDatetime[j]
leak.stop  <- singlesource.leaks$tc_ExpEndDatetime[j]
# If there is any overlap between estimated event and leak, remove
# the event
if (any(these.times >= leak.start & these.times <= leak.stop)){
spikes$events[this.spike] <- NA
to.remove[i] <- T
}
}
}
# Get event numbers after filtering out events that overlap with multisource leaks
event.nums <- na.omit(unique(spikes$events))
# Number of events after filtering multisource leaks
n.ints <- length(event.nums)
# Filter out all naive events that occurred during a multi-source release
rate.est.all.events    <- rate.est.all.events[!to.remove]
error.lower.all.events <- error.lower.all.events[!to.remove]
error.upper.all.events <- error.upper.all.events[!to.remove]
loc.est.all.events     <- loc.est.all.events[!to.remove]
all.durations          <- all.durations[!to.remove]
start.bounds           <- start.bounds[!to.remove]
end.bounds             <- end.bounds[!to.remove]
original.durations     <- original.durations[!to.remove]
# Initialize vector to hold estimated events that should be removed because
# they were combined with a naive event that overlapped with a multisource
# controlled release
to.remove <- vector(length = length(event.nums))
# Remove events that overlap with a source not included in the study
for (i in 1:length(event.nums)){
# Mask in this event
this.spike <- which(spikes$events == event.nums[i])
these.times <- spikes$time[this.spike]
if (i == length(event.nums)){
should.not.combine.check <- !any(loc.est.all.events[c(i-1)] == loc.est.all.events[i])
} else {
should.not.combine.check <- !any(loc.est.all.events[c(i-1, i+1)] == loc.est.all.events[i])
}
was.combined.check <- max(all.durations[[i]]) > (end.bounds[i] - start.bounds[i])
if (should.not.combine.check & was.combined.check){
spikes$events[this.spike] <- NA
to.remove[i] <- T
}
}
# Get event numbers after filtering out events that overlap with multisource leaks
event.nums <- na.omit(unique(spikes$events))
# Number of events after filtering multisource leaks
n.ints <- length(event.nums)
# Filter out all naive events that occurred during a multi-source release
rate.est.all.events    <- rate.est.all.events[!to.remove]
error.lower.all.events <- error.lower.all.events[!to.remove]
error.upper.all.events <- error.upper.all.events[!to.remove]
loc.est.all.events     <- loc.est.all.events[!to.remove]
all.durations          <- all.durations[!to.remove]
start.bounds           <- start.bounds[!to.remove]
end.bounds             <- end.bounds[!to.remove]
original.durations     <- original.durations[!to.remove]
} else if (run.type == 3){
singlesource.leaks <- leak.data
singlesource.leaks$rate.kg.hr <- singlesource.leaks$rate.kg.hr * 1000
singlesource.leaks$localization.est <- rep("short", nrow(singlesource.leaks))
colnames(singlesource.leaks) <- c("tc_ExpStartDatetime", "tc_ExpEndDatetime", "tc_C1MassFlow", "tc_ExpDurationHrs", "tc_EquipmentGroupID")
singlesource.leaks$tc_ExpStartDatetime <- as_datetime(as_datetime(singlesource.leaks$tc_ExpStartDatetime), tz = "US/Arizona")
singlesource.leaks$tc_ExpEndDatetime   <- as_datetime(as_datetime(singlesource.leaks$tc_ExpEndDatetime),   tz = "US/Arizona")
n.leaks <- nrow(singlesource.leaks)
diff.times <- vector(length = n.leaks-1)
for (i in 2:n.leaks){
diff.times[i-1] <- difftime(singlesource.leaks$tc_ExpStartDatetime[i],
singlesource.leaks$tc_ExpEndDatetime[i-1],
units = "mins")
}
to.combine <- diff.times < 15
# plot(diff.times, ylim = c(0,100))
# points(which(to.combine), diff.times[to.combine], col = "red")
new.leak.data <- singlesource.leaks[1,]
new.ind <- 1
to.average <- singlesource.leaks$tc_C1MassFlow[1]
for (i in 2:n.leaks){
if (to.combine[i-1]){
new.leak.data$tc_ExpEndDatetime[new.ind] <- singlesource.leaks$tc_ExpEndDatetime[i]
to.average <- c(to.average, singlesource.leaks$tc_C1MassFlow[i])
} else {
new.leak.data$tc_C1MassFlow[new.ind] <- mean(to.average)
new.ind <- new.ind + 1
new.leak.data <- rbind(new.leak.data, singlesource.leaks[i,])
to.average <- singlesource.leaks$tc_C1MassFlow[i]
}
}
new.leak.data$tc_ExpDurationHrs <- as.numeric(difftime(new.leak.data$tc_ExpEndDatetime, new.leak.data$tc_ExpStartDatetime, units = "hours"))
singlesource.leaks <- new.leak.data
n.leaks <- nrow(singlesource.leaks)
} # end if tree over run type
# Initialize matrix to hold information on which controlled release
# each naive event overlaps with.
# Rows are the METEC controlled releases, columns are naive events.
# True indicates an estimated event overlaps with a controlled release.
overlap.matrix <- matrix(NA, nrow = n.leaks, ncol = n.ints)
overlap.matrix[is.na(overlap.matrix)] <- F
# Fill in the overlap matrix. First loop over METEC emissions.
for (l in 1:n.leaks){
# Grab start and stop times of the METEC emission
leak.start <- singlesource.leaks$tc_ExpStartDatetime[l]
leak.end   <- singlesource.leaks$tc_ExpEndDatetime[l]
# Create minute sequence over duration of METEC emission
leak.times <- seq(leak.start, leak.end, by = "1 min")
leak.times <- round_date(leak.times, unit = "min")
# Loop over predicted events.
for (t in 1:n.ints){
# Create minute sequence over duration of this predicted event.
this.mask <- seq(min(which(spikes$events == event.nums[t])),
max(which(spikes$events == event.nums[t])))
event.times <- spikes$time[this.mask]
# Store overlap status
if (any(leak.times %in% event.times)){
overlap.matrix[l,t] <- T
}
}
}
# Initialize vectors to hold overlapping truth data
true.durations.matching <- true.rates.matching <- true.locs.matching <- vector(length = n.ints)
for (t in 1:n.ints){
# False positive naive events
if (length(which(overlap.matrix[,t])) == 0){
true.durations.matching[t] <- true.rates.matching[t] <- true.locs.matching[t] <- NA
# Naive events that overlap with a controlled release
} else if (length(which(overlap.matrix[,t])) == 1){
leak.ind <- which(overlap.matrix[,t])
this.true.duration <- as.numeric(difftime(singlesource.leaks$tc_ExpEndDatetime[leak.ind],
singlesource.leaks$tc_ExpStartDatetime[leak.ind],
units = "hour"))
true.rates.matching[t] <- singlesource.leaks$tc_C1MassFlow[leak.ind]
true.locs.matching[t] <- singlesource.leaks$tc_EquipmentGroupID[leak.ind]
true.durations.matching[t] <- this.true.duration
# Naive events that overlap with more than one controlled release
} else {
true.durations.matching[t] <- true.rates.matching[t] <- true.locs.matching[t] <- NA
print("ERROR: MORE THAN 1 MATCH")
}
}
if (F){
true.durations.matching <- true.rates.matching <- true.locs.matching <-
false.positive <- vector(length = n.ints)
for (t in 1:n.ints){
print(t)
par(mfrow = c(2,1))
par(mar = c(2,2,2,2))
this.mask <- seq(min(which(spikes$events == event.nums[t])),
max(which(spikes$events == event.nums[t])))
info.mask <- info.list[[which(names(info.list) == loc.est.all.events[t])]]
start.time <- spikes$time[this.mask][1]- hours(12)
end.time <- spikes$time[this.mask][length(this.mask)]+ hours(12)
ylim.vals <- c(event.nums[t] - 1, event.nums[t] + 1)
plot(times, spikes$events, pch = 19, xlim = c(start.time, end.time), ylim = ylim.vals,
col = NA, main = "")
mtext(loc.est.all.events[t], adj = 0)
abline(v = times[!is.na(info.mask$events)], col = alpha("royalblue", 0.25))
segments(x0 = spikes$time[this.mask], y0 = event.nums[t] - 0.25, y1 = event.nums[t] + 0.25)
for (tt in seq(1,n.ints)[-t]){
other.event.mask <- seq(min(which(spikes$events == event.nums[tt])),
max(which(spikes$events == event.nums[tt])))
segments(x0 = spikes$time[other.event.mask], y0 = event.nums[t] - 0.25, y1 = event.nums[t] + 0.25,
col = "gray44")
}
segments(x0 = start.bounds[t], y0 = event.nums[t] - 0.5, y1 = event.nums[t] + 0.5, col = "red", lwd = 2)
segments(x0 = end.bounds[t], y0 = event.nums[t] - 0.5, y1 = event.nums[t] + 0.5, col = "red", lwd= 2)
segments(x0 = spikes$time[this.mask], y0 = event.nums[t] - 0.25, y1 = event.nums[t] + 0.25)
if (length(which(overlap.matrix[,t])) == 0){
false.positive[t] <- T
true.durations.matching[t] <- true.rates.matching[t] <- true.locs.matching[t] <- NA
plot(1,1, main = t)
} else if (length(which(overlap.matrix[,t])) == 1){
leak.ind <- which(overlap.matrix[,t])[1]
this.true.duration <- as.numeric(difftime(singlesource.leaks$tc_ExpEndDatetime[leak.ind],
singlesource.leaks$tc_ExpStartDatetime[leak.ind],
units = "hour"))
true.rates.matching[t] <- singlesource.leaks$tc_C1MassFlow[leak.ind]
true.locs.matching[t] <- singlesource.leaks$tc_EquipmentGroupID[leak.ind]
true.durations.matching[t] <- this.true.duration
mtext(singlesource.leaks$tc_EquipmentGroupID[leak.ind], adj = 1)
segments(x0 = singlesource.leaks$tc_ExpStartDatetime[leak.ind],
x1 = singlesource.leaks$tc_ExpEndDatetime[leak.ind],
y0 = event.nums[t] + 0.5,
col = "forestgreen", lwd = 2)
xlim.vals <- c(0, ceiling(max(c(this.true.duration, all.durations[[t]]))))
hist(all.durations[[t]], xlim = xlim.vals, main = t, breaks = seq(0,max(xlim.vals)))
abline(v = this.true.duration, col = "orange", lwd = 8)
abline(v = mean(all.durations[[t]]), col = "blue", lwd = 6)
abline(v = original.durations[t], col = "purple", lwd = 4)
} else {
plot(1,1, main = t, pch = 19)
print("MORE THAN 1 MATCH")
}
}
}
duration.mean <- sapply(all.durations, mean)
duration.bound.lower <- sapply(all.durations, function(X) quantile(X, probs = 0.05))
duration.bound.upper <- sapply(all.durations, function(X) quantile(X, probs = 0.95))
p.errors <- (duration.mean - true.durations.matching) / true.durations.matching
percent.within.2x[number.of.sensors] <- sum(p.errors > -0.5 & p.errors < 1, na.rm = T) / sum(!is.na(p.errors))
proposed.fit <- lm(duration.mean ~ true.durations.matching - 1)
proposed.slope <- proposed.fit$coefficients[1]
proposed.fit$coefficients[1]
1-proposed.fit$coefficients[1]
100*(1-proposed.fit$coefficients[1])
# Clear environment
if(!is.null(dev.list())){dev.off()}
rm(list = ls())
# Import necessary libraries
library(lubridate)
library(zoo)
library(scales)
if (commandArgs()[1] == "RStudio"){
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
}
# Select experiment to run:
#   1) ADED 2022 controlled release experiment
#   2) ADED 2023 controlled release experiment
#   3) Stanford controlled release experiment
#   4) AMI case study
run.type <- 2
# Read in forward model data.
# Format must match output from the MAIN_1_simulate.R file, which can be found at:
# https://github.com/wsdaniels/DLQ/
if (run.type == 1){
data <- readRDS('../input_data/ADED2022_forward_model_output.RData')
} else if (run.type == 2){
data <- readRDS('../input_data/ADED2023_forward_model_output.RData')
} else if (run.type == 3){
data <- readRDS('../input_data/Stanford_forward_model_output.RData')
} else if (run.type == 4){
data <- readRDS('../input_data/case_study_forward_model_output.RData')
}
# Source helper files which contain functions that do most of the analysis
# source('/Users/wdaniels/Desktop/HELPER_spike_detection_algorithm.R')
source("https://raw.github.com/wsdaniels/DLQ/master/code/HELPER_spike_detection_algorithm.R")
source("../code/helper_functions.R")
# Where to save output
if (run.type == 1){
save.dir <- paste0("../output_data/ADED2022_duration_estimates.RData")
} else if (run.type == 2){
save.dir <- paste0("../output_data/ADED2023_duration_estimates.RData")
} else if (run.type == 3){
save.dir <- paste0("../output_data/Stanford_duration_estimates.RData")
} else if (run.type == 4){
save.dir <- "../output_data/case_study_duration_estimates_new.RData"
}
# Pull out sensor observations and replace NA's that are not on edge of
# the time series with interpolated values
obs <- na.approx(data$obs, na.rm = F)
source.names <- names(sims)
# Clear environment
if(!is.null(dev.list())){dev.off()}
rm(list = ls())
# Import necessary libraries
library(lubridate)
library(zoo)
library(scales)
if (commandArgs()[1] == "RStudio"){
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
}
# Select experiment to run:
#   1) ADED 2022 controlled release experiment
#   2) ADED 2023 controlled release experiment
#   3) Stanford controlled release experiment
#   4) AMI case study
run.type <- 2
# Read in forward model data.
# Format must match output from the MAIN_1_simulate.R file, which can be found at:
# https://github.com/wsdaniels/DLQ/
if (run.type == 1){
data <- readRDS('../input_data/ADED2022_forward_model_output.RData')
} else if (run.type == 2){
data <- readRDS('../input_data/ADED2023_forward_model_output.RData')
} else if (run.type == 3){
data <- readRDS('../input_data/Stanford_forward_model_output.RData')
} else if (run.type == 4){
data <- readRDS('../input_data/case_study_forward_model_output.RData')
}
# Source helper files which contain functions that do most of the analysis
# source('/Users/wdaniels/Desktop/HELPER_spike_detection_algorithm.R')
source("https://raw.github.com/wsdaniels/DLQ/master/code/HELPER_spike_detection_algorithm.R")
source("../code/helper_functions.R")
# Where to save output
if (run.type == 1){
save.dir <- paste0("../output_data/ADED2022_duration_estimates.RData")
} else if (run.type == 2){
save.dir <- paste0("../output_data/ADED2023_duration_estimates.RData")
} else if (run.type == 3){
save.dir <- paste0("../output_data/Stanford_duration_estimates.RData")
} else if (run.type == 4){
save.dir <- "../output_data/case_study_duration_estimates_new.RData"
}
# Pull out sensor observations and replace NA's that are not on edge of
# the time series with interpolated values
obs <- na.approx(data$obs, na.rm = F)
# Number of sensors
n.r <- ncol(obs)
# Pull out time stamps of observations and simulations
times <- data$times
# Pull out the simulation predictions
sims <- data[5:length(data)]
# Grab source info
n.s <- length(sims)
source.names <- names(sims)
# Remove background from CMS observations
obs <- remove.background(times, obs,
going.up.threshold = 0.25, amp.threshold = 0.75,
gap.time = 30)
going.up.threshold = 0.25
amp.threshold = 0.75
gap.time = 30
# Skip sensors that have only NA values
to.use <- which(apply(obs, 2, function(X) !all(is.na(X))))
to.use
j=1
# Grab observations from this sensor
this.raw.obs <- obs[,j]
# Remove the NA's at the beginning and end of the time series
# (some sensors start late or end early)
to.keep <- !is.na(this.raw.obs)
trimmed.obs <- this.raw.obs[to.keep]
trimmed.obs
trimmed.times <- times[to.keep]
trimmed.trimes
trimmed.times
